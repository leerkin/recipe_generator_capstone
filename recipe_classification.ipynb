{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:44:31.129804Z",
     "start_time": "2024-06-30T20:44:30.623297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import cProfile\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.impute import KNNImputer\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "from model_functions import logistic_test, svm_test, nb_test, rf_test, gb_test, ensemble_test, stacking_test, run_model\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib"
   ],
   "id": "1d465c796fa5428e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "249f9f0041fa6707",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''def main():\n",
    "    # Read the datasets\n",
    "    ingredients_df = pd.read_csv('data/ingredients_df_balanced.csv')\n",
    "    flavour_df = pd.read_csv('data/flavour_df_balanced.csv')\n",
    "    region_profile_df = pd.read_csv('data/region_profile_df_balanced.csv')\n",
    "    merged_df = pd.read_csv('data/merged_df.csv')\n",
    "\n",
    "    # Separate features and target\n",
    "    X = merged_df.drop(columns=['id', 'region'])\n",
    "    y = merged_df['region']\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=0.95)  # retain 95% of variance\n",
    "    X = pca.fit_transform(X)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define models to test\n",
    "    models = [logistic_test, svm_test, nb_test, rf_test, gb_test, knn_test, ensemble_test, stacking_test]\n",
    "\n",
    "    # Run models in parallel with progress bar\n",
    "    results = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(run_model, model, X_train_scaled, X_test_scaled, y_train, y_test): model for model in models}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Running models\"):\n",
    "            results.append(future.result())\n",
    "\n",
    "    # Display results\n",
    "    for model_name, accuracy in results:\n",
    "        print(f\"{model_name} - Accuracy: {accuracy}\")\n",
    "\n",
    "    # Plot confusion matrix with Ensemble model\n",
    "    ensemble = StackingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('svc', SVC(kernel='linear', probability=True)),\n",
    "        ('rf', RandomForestClassifier()),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ], final_estimator=GradientBoostingClassifier())\n",
    "    ensemble.fit(X_train_scaled, y_train)\n",
    "    y_pred = ensemble.predict(X_test_scaled)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix based on Ensemble Model')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(np.unique(y)))\n",
    "    plt.xticks(tick_marks, np.unique(y), rotation=45)\n",
    "    plt.yticks(tick_marks, np.unique(y))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "'''"
   ],
   "id": "d8b0d8413f83c640",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Read the datasets\n",
    "    merged_df = pd.read_csv('data/merged_df_imputed.csv')\n",
    "\n",
    "    # Separate features and target\n",
    "    X = merged_df.drop(columns=['id', 'region'])\n",
    "    y = merged_df['region']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define models to test\n",
    "    models = [logistic_test, svm_test, nb_test, rf_test, gb_test, ensemble_test, stacking_test]\n",
    "\n",
    "    results = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(run_model, model, X_train_scaled, X_test_scaled, y_train, y_test): model for model in models}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Running models\"):\n",
    "            results.append(future.result())\n",
    "\n",
    "    # Display results\n",
    "    for model_name, accuracy in results:\n",
    "        print(f\"{model_name} - Accuracy: {accuracy}\")\n",
    "\n",
    "    # Plot confusion matrix with Ensemble model\n",
    "    ensemble = StackingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('svc', SVC(kernel='linear', probability=True)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ], final_estimator=GradientBoostingClassifier())\n",
    "    ensemble.fit(X_train_scaled, y_train)\n",
    "    y_pred = ensemble.predict(X_test_scaled)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix based on Ensemble Model')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(np.unique(y)))\n",
    "    plt.xticks(tick_marks, np.unique(y), rotation=45)\n",
    "    plt.yticks(tick_marks, np.unique(y))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "b3f58cbc98acf465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "1518e3b653af83c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T21:00:57.525098Z",
     "start_time": "2024-06-30T21:00:57.063414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # Read the datasets\n",
    "merged_df = pd.read_csv('data/merged_df_imputed.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = merged_df.drop(columns=['id', 'region'])\n",
    "y = merged_df['region']"
   ],
   "id": "1be5b7c760a24ab4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T21:01:00.574296Z",
     "start_time": "2024-06-30T21:01:00.432298Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "eaab5ddd42185ee7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T21:01:06.913330Z",
     "start_time": "2024-06-30T21:01:06.896659Z"
    }
   },
   "cell_type": "code",
   "source": "X_train",
   "id": "77b94aa2f80b205d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      compound_count  ingredient_count  almond  angelica  anise  anise_seed  \\\n",
       "3593             245                 6     1.0       0.0    0.0         0.0   \n",
       "794              430                13     0.0       0.0    0.0         0.0   \n",
       "6568             229                 8     0.0       0.0    0.0         0.0   \n",
       "1533             272                 8     0.0       0.0    0.0         0.0   \n",
       "1558             215                 9     0.0       0.0    0.0         0.0   \n",
       "...              ...               ...     ...       ...    ...         ...   \n",
       "3772             309                 9     0.0       0.0    0.0         0.0   \n",
       "5191             228                 7     0.0       0.0    0.0         0.0   \n",
       "5226             202                 4     0.0       0.0    0.0         0.0   \n",
       "5390             282                14     0.0       0.0    0.0         0.0   \n",
       "860              207                10     0.0       0.0    0.0         0.0   \n",
       "\n",
       "      apple  apple_brandy  apricot  armagnac  ...  1097  1098  1099  1100  \\\n",
       "3593    0.0           0.0      0.0       0.0  ...   0.0   0.0   0.0   0.0   \n",
       "794     0.0           0.0      0.0       0.0  ...   1.0   1.0   0.0   0.0   \n",
       "6568    0.0           0.0      0.0       0.0  ...   0.0   0.0   0.0   0.0   \n",
       "1533    0.0           0.0      0.0       0.0  ...   0.0   0.0   0.0   0.0   \n",
       "1558    0.0           0.0      0.0       0.0  ...   0.0   0.0   0.0   0.0   \n",
       "...     ...           ...      ...       ...  ...   ...   ...   ...   ...   \n",
       "3772    0.0           0.0      0.0       0.0  ...   0.0   0.0   0.0   0.0   \n",
       "5191    0.0           0.0      0.0       0.0  ...   0.0   1.0   0.0   0.0   \n",
       "5226    0.0           0.0      0.0       0.0  ...   0.0   1.0   0.0   0.0   \n",
       "5390    0.0           0.0      0.0       0.0  ...   0.0   0.0   0.0   0.0   \n",
       "860     0.0           0.0      0.0       0.0  ...   0.0   1.0   0.0   0.0   \n",
       "\n",
       "      1101  1102  1103  1104  1105  1106  \n",
       "3593   1.0   1.0   0.0   1.0   0.0   0.0  \n",
       "794    1.0   3.0   0.0   4.0   0.0   1.0  \n",
       "6568   0.0   2.0   0.0   3.0   0.0   0.0  \n",
       "1533   0.0   0.0   0.0   3.0   0.0   0.0  \n",
       "1558   0.0   0.0   0.0   1.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...  \n",
       "3772   1.0   1.0   0.0   1.0   0.0   0.0  \n",
       "5191   0.0   1.0   0.0   2.0   0.0   0.0  \n",
       "5226   0.0   2.0   0.0   1.0   0.0   0.0  \n",
       "5390   0.0   3.0   0.0   1.0   0.0   0.0  \n",
       "860    0.0   0.0   0.0   2.0   0.0   0.0  \n",
       "\n",
       "[5676 rows x 1492 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound_count</th>\n",
       "      <th>ingredient_count</th>\n",
       "      <th>almond</th>\n",
       "      <th>angelica</th>\n",
       "      <th>anise</th>\n",
       "      <th>anise_seed</th>\n",
       "      <th>apple</th>\n",
       "      <th>apple_brandy</th>\n",
       "      <th>apricot</th>\n",
       "      <th>armagnac</th>\n",
       "      <th>...</th>\n",
       "      <th>1097</th>\n",
       "      <th>1098</th>\n",
       "      <th>1099</th>\n",
       "      <th>1100</th>\n",
       "      <th>1101</th>\n",
       "      <th>1102</th>\n",
       "      <th>1103</th>\n",
       "      <th>1104</th>\n",
       "      <th>1105</th>\n",
       "      <th>1106</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>245</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>430</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>229</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>272</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>215</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>309</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>228</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>202</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>282</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>207</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5676 rows × 1492 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T21:01:33.843671Z",
     "start_time": "2024-06-30T21:01:31.456717Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.to_csv('data/X_train.csv', index=False)",
   "id": "cf2f8339fad19a61",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T20:44:47.739749Z",
     "start_time": "2024-06-30T20:44:47.733949Z"
    }
   },
   "cell_type": "code",
   "source": "joblib.dump(scaler, 'scaler.pkl')",
   "id": "a9f1b12b51d8aa72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ensemble_test(X_train, X_test, y_train, y_test):\n",
    "    estimators = [\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('svc', SVC(kernel='linear', probability=True)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]\n",
    "    ensemble = StackingClassifier(estimators=estimators, final_estimator=GradientBoostingClassifier())\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    y_pred = ensemble.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return 'Ensemble', acc"
   ],
   "id": "8164c946999ddb3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ensemble_test(X_train_scaled, X_test_scaled, y_train, y_test)",
   "id": "68faf6e8710793b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_ensemble_model(X_train, y_train):\n",
    "    estimators = [\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('svc', SVC(kernel='linear', probability=True)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]\n",
    "    ensemble = StackingClassifier(estimators=estimators, final_estimator=GradientBoostingClassifier())\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    return ensemble\n",
    "\n",
    "# Train the model\n",
    "ensemble = train_ensemble_model(X_train_scaled, y_train)"
   ],
   "id": "8f00d12a556f40e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "joblib.dump(ensemble, 'pretrained_ensemble_model.pkl')",
   "id": "b9bb1b2dcfc3b3a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "def cross_validate_ensemble(X, y):\n",
    "    estimators = [\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('svc', SVC(kernel='linear', probability=True)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]\n",
    "\n",
    "    ensemble = StackingClassifier(estimators=estimators, final_estimator=GradientBoostingClassifier())\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(ensemble, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Fit the model on the entire dataset\n",
    "    ensemble.fit(X, y)\n",
    "\n",
    "    return ensemble, cv_scores\n",
    "\n",
    "cross_validate_ensemble(X_train_scaled, y_train)"
   ],
   "id": "c21341d551d934e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged_df = pd.read_csv('data/merged_df_imputed.csv')",
   "id": "2df4efcde6da2e85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df = pd.read_csv('data/ingredients_df.csv')",
   "id": "b7a24ec8b07d86e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_id_column(df, column_name='id', start=1):\n",
    "    \"\"\"\n",
    "    Add an ID column to the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to which the ID column will be added.\n",
    "    - column_name: Name of the ID column (default is 'id').\n",
    "    - start: Starting value of the ID (default is 1).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with the added ID column.\n",
    "    \"\"\"\n",
    "    df.insert(0, column_name, range(start, start + len(df)))\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming ingredients_df is your DataFrame\n",
    "ingredients_df = add_id_column(ingredients_df)\n"
   ],
   "id": "8e2c51a169b476e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter North American cuisine\n",
    "north_american_df = ingredients_df[ingredients_df['region'] == 'north_american']\n",
    "\n",
    "# Randomly sample 5000 observations\n",
    "north_american_sampled_df = north_american_df.sample(n=3000, random_state=42)\n",
    "\n",
    "# Combine the sampled North American data with the rest of the DataFrame\n",
    "other_df = ingredients_df[ingredients_df['region'] != 'north_american']\n",
    "ingredients_df = pd.concat([other_df, north_american_sampled_df], ignore_index=True)"
   ],
   "id": "35b92819b32dce1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df",
   "id": "3c8c87b6c1be9f81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assuming ingredients_df is already defined and loaded\n",
    "\n",
    "# Find the size of the median class\n",
    "median_class_size = ingredients_df['region'].value_counts().median()\n",
    "\n",
    "# Separate each region into individual DataFrames\n",
    "dfs = [df for _, df in ingredients_df.groupby('region')]\n",
    "\n",
    "# Initialize KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Initialize OneHotEncoder for categorical data\n",
    "categorical_cols = ingredients_df.select_dtypes(include=['object']).columns\n",
    "numeric_cols = ingredients_df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Resample or impute each region to the median class size\n",
    "balanced_dfs = []\n",
    "for df in dfs:\n",
    "    if len(df) < median_class_size:\n",
    "        # Resample with replacement to median size\n",
    "        resampled_df = resample(df, replace=True, n_samples=int(median_class_size), random_state=42)\n",
    "\n",
    "        # Encode categorical columns\n",
    "        encoded_categorical = ohe.fit_transform(resampled_df[categorical_cols])\n",
    "        encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=ohe.get_feature_names_out(categorical_cols))\n",
    "\n",
    "        # Combine numeric and encoded categorical columns\n",
    "        combined_df = pd.concat([resampled_df[numeric_cols].reset_index(drop=True), encoded_categorical_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # Apply KNN Imputer\n",
    "        imputed_combined = knn_imputer.fit_transform(combined_df)\n",
    "        imputed_df = pd.DataFrame(imputed_combined, columns=combined_df.columns)\n",
    "\n",
    "        # Separate back the numeric and categorical columns\n",
    "        imputed_numeric = imputed_df[numeric_cols]\n",
    "        imputed_categorical = pd.DataFrame(ohe.inverse_transform(imputed_df[encoded_categorical_df.columns]), columns=categorical_cols)\n",
    "\n",
    "        # Combine into final DataFrame\n",
    "        final_df = pd.concat([imputed_numeric, imputed_categorical], axis=1)\n",
    "        balanced_dfs.append(final_df)\n",
    "    else:\n",
    "        # Resample without replacement to median size\n",
    "        resampled_df = resample(df, replace=False, n_samples=int(median_class_size), random_state=42)\n",
    "        balanced_dfs.append(resampled_df)\n",
    "\n",
    "# Combine the DataFrames\n",
    "balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "# Update ingredients_df with the balanced DataFrame\n",
    "ingredients_df = balanced_df.copy()\n",
    "\n",
    "# Verify the new counts\n",
    "balanced_region_counts = ingredients_df['region'].value_counts()\n",
    "balanced_country_counts = ingredients_df['country'].value_counts()\n",
    "\n",
    "# Plot the data for regions\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=balanced_region_counts.values, y=balanced_region_counts.index, palette='viridis')\n",
    "plt.title('Number of Entries per Region (Balanced)')\n",
    "plt.xlabel('Number of Entries')\n",
    "plt.ylabel('Region')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the data for countries\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=balanced_country_counts.values, y=balanced_country_counts.index, palette='viridis')\n",
    "plt.title('Number of Entries per Country (Balanced)')\n",
    "plt.xlabel('Number of Entries')\n",
    "plt.ylabel('Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "de51f87f735705df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def flavor_profile(ingredients_df, ingr_df, comp_df, ingr_comp_df):\n",
    "    ingredients_df.reset_index(drop=True, inplace=True)\n",
    "    ingredients_df['id'] = ingredients_df.index\n",
    "\n",
    "    function_df = ingredients_df.copy()\n",
    "    undesired_columns = ['region', 'id']  # Keep 'id' out of calculations\n",
    "    function_df.drop(undesired_columns, axis=1, inplace=True)\n",
    "    sorted_ingredients = function_df.columns\n",
    "\n",
    "    ingr_total = ingr_comp_df.merge(ingr_df, how='right', on='ingredient_id')\n",
    "    ingr_total = ingr_total.merge(comp_df, how='right', on='compound_id')\n",
    "\n",
    "    ingr_pivot = pd.crosstab(ingr_total['ingredient_name'], ingr_total['compound_id'])\n",
    "    ingr_flavor = ingr_pivot.reindex(sorted_ingredients).fillna(0)\n",
    "\n",
    "    df_flavor = pd.DataFrame(np.dot(function_df.values, ingr_flavor.values), index=ingredients_df.index)\n",
    "    df_flavor['region'] = ingredients_df['region']\n",
    "    df_flavor['id'] = ingredients_df['id']  # Add 'id' column\n",
    "\n",
    "    # Reorder columns to have 'id' first\n",
    "    cols = ['id'] + [col for col in df_flavor.columns if col != 'id']\n",
    "    df_flavor = df_flavor[cols]\n",
    "\n",
    "    # Reorder columns to have 'region' and 'country' first\n",
    "    column_order = ['region'] + [col for col in df_flavor.columns if col not in ['region']]\n",
    "    df_flavor = df_flavor[column_order]\n",
    "\n",
    "    return df_flavor\n",
    "\n",
    "def add_flavour_profile(ingredients_df, flavour_df):\n",
    "    # Ensure 'id' is included as a unique identifier\n",
    "    ingredients_df['id'] = range(len(ingredients_df))\n",
    "    flavour_df['id'] = ingredients_df['id']\n",
    "\n",
    "    # Ensure that 'flavour_df' and 'ingredients_df' have the same index\n",
    "    flavour_df.index = ingredients_df.index\n",
    "    ingredients_df.index = ingredients_df.index\n",
    "\n",
    "    # Create a new DataFrame\n",
    "    region_recipe_df = pd.DataFrame()\n",
    "    region_recipe_df['id'] = ingredients_df['id']\n",
    "    region_recipe_df['region'] = ingredients_df['region']\n",
    "\n",
    "    # Calculate the total count of compounds per recipe from flavour_df\n",
    "    region_recipe_df['compound_count'] = np.count_nonzero(flavour_df.values, axis=1)\n",
    "\n",
    "    # Calculate the total count of ingredients per recipe from ingredients_df\n",
    "    ingredient_columns = [col for col in ingredients_df.columns if col not in ['id', 'region']]\n",
    "    region_recipe_df['ingredient_count'] = np.count_nonzero(ingredients_df[ingredient_columns].values, axis=1)\n",
    "\n",
    "    return region_recipe_df\n",
    "\n"
   ],
   "id": "92586185e42efa1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df",
   "id": "82645f73c717f4ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df.to_csv('data/ingredients_df_imputed.csv', index=False)",
   "id": "118be1316aaf3bec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "flavour_df.to_csv('data/flavour_df_imputed.csv', index=False)\n",
    "region_profile_df.to_csv('data/region_profile_df_imputed.csv', index=False)"
   ],
   "id": "3cc184abd0cc8990",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "comp_path = 'data/flavor_network_data/ingr_comp/comp_info.tsv'\n",
    "comp_tsv = pd.read_csv(comp_path, delimiter='\\t')\n",
    "\n",
    "comp_df = pd.DataFrame(data = comp_tsv)\n",
    "comp_columns = ['compound_id', 'compound_name', 'CAS_number']\n",
    "comp_df.columns = comp_columns\n",
    "\n",
    "ingr_path = 'data/flavor_network_data/ingr_comp/ingr_info.tsv'\n",
    "ingr_tsv = pd.read_csv(ingr_path, delimiter='\\t')\n",
    "\n",
    "ingr_df = pd.DataFrame(data = ingr_tsv)\n",
    "ingr_columns = ['ingredient_id', 'ingredient_name', 'ingredient_category']\n",
    "ingr_df.columns = ingr_columns\n",
    "\n",
    "ingr_comp_pathh = 'data/flavor_network_data/ingr_comp/ingr_comp.tsv'\n",
    "ingr_comp_tsv = pd.read_csv(ingr_comp_pathh, delimiter='\\t')\n",
    "\n",
    "ingr_comp_df = pd.DataFrame(data = ingr_comp_tsv)\n",
    "ingr_comp_df.rename(columns={\n",
    "    '# ingredient id': 'ingredient_id',\n",
    "    'compound id': 'compound_id'\n",
    "}, inplace=True)"
   ],
   "id": "702092f2e3402530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df = pd.read_csv('data/ingredients_df.csv')",
   "id": "5ca5ab2269c5a4c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ingredients_df = pd.read_csv('data/ingredients_df.csv')\n",
    "flavour_df = pd.read_csv('data/flavour_df_imputed.csv')\n",
    "region_profile_df = pd.read_csv('data/region_profile_df_imputed.csv')"
   ],
   "id": "f55bc79372abab03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df",
   "id": "a6fdb0ca228f7bab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df = ingredients_df.drop(columns=['country'])",
   "id": "fcbf7642c3b6c084",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df",
   "id": "3dc5ad83d8e42315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "flavour_df = flavor_profile(ingredients_df, ingr_df, comp_df, ingr_comp_df)",
   "id": "d0deac869c6cc85e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "region_profile_df = add_flavour_profile(ingredients_df, flavour_df)",
   "id": "e39af0f449844f46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop the 'country' column from ingredients_df\n",
    "flavour_df = flavour_df.drop(columns=['region'])\n",
    "\n",
    "# Drop the 'country' column from flavour_df\n",
    "ingredients_df = ingredients_df.drop(columns=['region'])"
   ],
   "id": "14d74d849d53b6c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df",
   "id": "28ba58e1ad82861c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reset the index to ensure it's unique and sequential, if needed\n",
    "ingredients_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add an ID column based on the new index\n",
    "ingredients_df['id'] = ingredients_df.index"
   ],
   "id": "635fdc516cd71ec1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure 'id' is the first column\n",
    "cols = list(ingredients_df.columns)\n",
    "cols.insert(0, cols.pop(cols.index('id')))\n",
    "\n",
    "# Ensure 'region' is the second column\n",
    "cols.insert(1, cols.pop(cols.index('region')))\n",
    "\n",
    "# Reorder the DataFrame\n",
    "ingredients_df = ingredients_df[cols]"
   ],
   "id": "24948d611243b49b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ingredients_df",
   "id": "bbdca549653ebe84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "flavour_df",
   "id": "50d782b822b7bc3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Merge the dataframes on 'id'\n",
    "merged_df = region_profile_df.merge(ingredients_df, on='id').merge(flavour_df, on='id')"
   ],
   "id": "3b70053d4451bab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged_df",
   "id": "46142b54211344c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged_df.to_csv('data/merged_df_imputed.csv', index=False)",
   "id": "8410867dc749346d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.Series(y_pred)",
   "id": "ee71fdf4c6fcdd55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(10, 10))\n",
    "cuisine_labels = sorted(y['region'].unique())\n",
    "plot_confusion_matrix_1(cm_normalized, cuisine_labels, title='Confusion Matrix based on ingredients')\n",
    "plt.show()"
   ],
   "id": "65012d5c73b58935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5511ae0fc5941bd4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
